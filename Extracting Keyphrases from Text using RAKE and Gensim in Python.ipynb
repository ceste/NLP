{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rake-nltk\n",
      "  Downloading rake_nltk-1.0.4.tar.gz (7.6 kB)\n",
      "Requirement already satisfied: nltk in /home/cst/anaconda3/lib/python3.7/site-packages (from rake-nltk) (3.4.5)\n",
      "Requirement already satisfied: six in /home/cst/anaconda3/lib/python3.7/site-packages (from nltk->rake-nltk) (1.12.0)\n",
      "Building wheels for collected packages: rake-nltk\n",
      "  Building wheel for rake-nltk (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rake-nltk: filename=rake_nltk-1.0.4-py2.py3-none-any.whl size=7819 sha256=2eb6da283f65f74e0d3f81074fa03fa3aeab9a5d11e444481e94bce94905c210\n",
      "  Stored in directory: /home/cst/.cache/pip/wheels/7c/d9/8a/b8a9244fa89a07f288f9fe006aafc79d93fceb58496c29b606\n",
      "Successfully built rake-nltk\n",
      "Installing collected packages: rake-nltk\n",
      "Successfully installed rake-nltk-1.0.4\n"
     ]
    }
   ],
   "source": [
    "# !pip install python-rake\n",
    "# !pip install rake-nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import RAKE\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtitles = '''\n",
    "                AI Platform Pipelines has two major parts: (1) the infrastructure for deploying and running structured AI workflows that are integrated with Google Cloud Platform services and (2) the pipeline tools for building, debugging, and sharing pipelines and components. The service runs on a Google Kubernetes cluster that’s automatically created as a part of the installation process, and it’s accessible via the Cloud AI Platform dashboard. With AI Platform Pipelines, developers specify a pipeline using the Kubeflow Pipelines software development kit (SDK), or by customizing the TensorFlow Extended (TFX) Pipeline template with the TFX SDK. This SDK compiles the pipeline and submits it to the Pipelines REST API server, which stores and schedules the pipeline for execution.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_dir = 'data/SmartStoplist.txt'\n",
    "rake_object = RAKE.Rake(stop_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sort_Tuple(tup):\n",
    "    tup.sort(key=lambda x:x[1])\n",
    "    return tup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keywords: [('installation process', 4.0), ('tensorflow extended', 4.0), ('sharing pipelines', 5.4), ('google kubernetes cluster', 9.5), ('ai platform pipelines', 10.4), ('google cloud platform services', 15.0), ('cloud ai platform dashboard', 15.0), ('pipelines rest api server', 15.4), ('running structured ai workflows', 15.5), ('kubeflow pipelines software development kit', 23.4)]\n"
     ]
    }
   ],
   "source": [
    "keywords = Sort_Tuple(rake_object.run(subtitles))[-10:]\n",
    "print('keywords:',keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rake_nltk import Rake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = Rake()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.extract_keywords_from_text(subtitles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kubeflow pipelines software development kit',\n",
       " 'running structured ai workflows',\n",
       " 'pipelines rest api server',\n",
       " 'cloud ai platform dashboard',\n",
       " 'google cloud platform services',\n",
       " 'ai platform pipelines',\n",
       " 'google kubernetes cluster',\n",
       " 'two major parts',\n",
       " 'sharing pipelines',\n",
       " 'tensorflow extended']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.get_ranked_phrases()[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.summarization import keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(keywords())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
